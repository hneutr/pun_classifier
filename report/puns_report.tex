\documentclass{article}

\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{Pun Classification and Location}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
   Amir Kashipazha \\
   Masters Student in Computer Science\\
   University of Colorado Boulder\\
   \texttt{amirhossein.kashipazha@colorado.edu} \\
  %% examples of more authors
   \And
   Brandon Boylan-Peck \\
   Masters Student in Computer Science\\
   University of Colorado Boulder \\
   \texttt{brbo9266@colorado.edu} \\
   \And
   Cathlyn Stone \\
   Masters Student in Computer Science\\
   University of Colorado Boulder \\
   \texttt{cathlyn.stone@colorado.edu} \\
   \And
   Kenneth Hunter Wapman \\
   Masters Student in Computer Science\\
   University of Colorado Boulder \\
   \texttt{kennethwapman@colorado.edu} \\
   \And
   Shantanu Karnwal \\
   Masters Student in Computer Science\\
   University of Colorado Boulder \\
   \texttt{shantanu.karnwal@colorado.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
	Pun identification and location are challenging natural language processing 
	tasks. We implemented several algorithms for both, with results which 
	were competitive with the systems presented in the SemEval conference, where
	those tasks were originally defined.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem Description
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview}
A pun is a form of wordplay within the class of textual phenomena involving
ambiguity where multiple meanings of a word or set of words are played upon to
create a rhetorical effect. They occur in many domains: in advertising, puns
are used to connect a product with a desirable quality; in literature, puns are
used to add a level of meaning and implicaiton; in conversation, puns add humor
to the topic at hand.

Understanding a pun often requires the integration of information from a large
number of contexts; syntax, morphology, semantics, and pragmatics, and context
can all come into play. There are many types of puns:

\begin{itemize}
	\item{\textbf{Homographic}: A pun involving two words whose spellings are
		the same but have different meanings.
		\begin{center}
		\emph{I used to be a banker but I lost \textbf{interest}.}
		\end{center}
	}
	\item{\textbf{Heterographic}: A pun involving two or more words which sound
		alike but are spelled differently. Sometimes termed `homophonic'.
		\begin{center}
		\emph{Two construction workers had a \textbf{stairing} contest.}
		\end{center}
	}
	\item{\textbf{Compound}: A sentence containing two or more puns.
		\begin{center}
		\emph{Why can a man never starve in the Great Desert? Because he can
		eat the sand which is there. But what brought the sandwiches there?
		Why, Noah sent Ham, and his descendants mustered and bred.} - Richard Whately
		\end{center}
	}
	\item{\textbf{Visual}: A pun realized by an image. See figure \ref{fig:visual_pun}\footnote{\url{https://digitalsynopsis.com/design/punny-pixels-illustrated-puns-visual-wordplay/}}.
	}
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=5cm]{figures/visual_pun.jpg}
\caption{A visual Pun.}\label{fig:visual_pun}
\end{figure}

We focused on two types: homographic puns and heterographic puns. 

\subsection{Motivation}

It would be desirable for machines to be able to identify, comprend, and
generate puns for a number of reasons:

\begin{itemize}
	\item A pun generator could facilitate slogan generation, which would be
		useful in advertising and dialogue agents.
	\item A pun identifier could be used to improve machine translations, where
		literal translation of puns rarely conveys the original text's message.
	\item A pun comprehension system would be helpful for large-scale textual
		analyses of the kind increasingly being employed in the digital
		humanities.
\end{itemize}

We worked on two related tasks: pun detection and pun location.

\subsection{Dataset}

We trained and tested our algorithms on the test sets used to appraise the
competing systems in SemEval2017 Task 7. The dataset for each of the tasks we
worked on was split into two parts: homographic and heterographic. We modified
the characteristics of these datasets to address some issues we found. Namely,
the original datasets for the detection tasks for both heterographic and
homographic puns had a pun/non-pun split of 70/30. The non-pun sentences for
each type of pun were, however, non-overlapping, so we simply modified each pun
type's dataset to include \emph{all} non-puns from either dataset, giving us a
more even ratio of pun/non-pun.\ 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pun Detection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pun Detection}\label{pun_detection}

Pun detection is a binary classification problem: given a sequence of words (a
sentence), determine whether or not that sentence contains a pun. 

We tried a number of different approaches to this problem, but found that the
best approach was a Bidirectional LSTM.

\subsection{Baseline: Multinomial Naive Bayes}

A random approach to this task will have a $\sim$$50\%$ of being correct.
However, we wanted to determine whether we could do better than random with a
simple classifier. To that end, we implemented a simple Multinomial Naive Bayes
classifier. 

\subsection{Linear SVM}

We implemented a linear SVM and trained it using Stochastic Gradient Descent.
The intution behind this was that an SVM would be able to incorporate arbitrary
features, and that the data would be linearly separable with the right features.
This approach worked quite well, but was slightly less effective than our neural
net approach.

\subsubsection{Feature Comparison}

We tried to improve the Linear SVM's performance by including a variety of
features (Table \ref{tab:List_of_features}), as well as using different
combinations of those features.  Features were added to the code one by one to
find whether they improve accuracy. In general, accuracy improved with the
addition of more features.

We determined which combinations of features would improve the algorithm the
most using a heuristic based approach to select the features to include in
combinations. Testing all combinations of features was not feasible: with 16
features and 65535 combinations of those features, testing each one at a runtime
of 60 seconds per run would have taken 1092 hours. After identifying 11
features (0-10 in table \ref{tab:List_of_features}), we ran a brute-force search
of the 2047 combinations (over both the heterographic and homographic datasets)
of those features to determine which was best.

\begin{table}
\caption{Features by Index}\label{tab:List_of_features} 
\begin{center}
\begin{tabular}{l l l l} 
\toprule
\textbf{\#} &\textbf{Feature Name}		& \textbf{\#} &	\textbf{Feature Name}\\
\midrule
0	&	Lesk Algorithm					& 8		&	Idiom is in Pun or Not\\
1	&	Pos								& 9		&	Antonyms is in Pun or Not\\
2	&	tfidf							& 10	&	Homonym is in Pun or Not\\
3	&	Embeddings						& 11	&	Bigram\\
4	&	Unigram							& 12	&	Trigram\\
5	&	Number of Homophone in Pun		& 13	&	Positives\\
6	&	Number of Each Homophone in Pun & 14	&	Negatives\\
7	&	Homophone is in Pun or Not		& 15	&	All or First Caps\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{figures/Accuracy_on_Training_Set_for_Homographic_Pun.png}
  \caption{Homographic Pun Feature Comparison Training Set Accuracy}\label{fig:ACC_Train_Homo}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{figures/Accuracy_on_Test_Set_for_Homographic_Pun.png}
  \caption{Homographic Pun Feature Comparison Test Set Accuracy}\label{fig:ACC_Test_Homo}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{figures/Accuracy_on_Test_Set_for_Heterographic_Pun.png}
  \caption{Heterographic Pun Feature Comparison Test Set Accuracy}\label{fig:ACC_Test_Hetero}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{figures/Accuracy_on_Training_Set_for_Heterographic_Pun.png}
  \caption{Heterographic Pun Feature Comparison Training Set Accuracy}\label{fig:ACC_Train_Hetero}
\end{figure}

\subsection{Bidirectional LSTM}

Despite having a fairly small dataset, we decided to try a neural net on the
problem. We chose an LSTM because of its ability to deal with variable-sentence
lengths, as (somewhat obviously) our dataset featured sentences of different
lengths. This was our most effective approach. We used glove embeddings with a
length of 300.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pun Location
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pun Location}
\label{pun_location}

Pun location is a less straightforward task than pun detection: given a set of
words known to contain a pun, identify the pun word. Though this problem is less
amenable to traditional machine learning algorithms, it can be rephrased to be
more similar to one: for each word in the sentence, determine the likelihood
that that word is a pun given the rest of the sentence. Then, output the word
which is most likely to be a pun based on the output of the previous search.  A
random approach to this dataset does not work particularly well; it will have an
accuracy of about 1 divided by the average length of the sentence.
 
\subsection{Baselines}

We tried the \textbf{Multinomial Naive Bayes} classifier used as a baseline in
the detection task. It did not perform well.


We also implemented a \textbf{Heuristic Decision Tree} model to locate a pun
word. The intuition behind this model was that much of the time, the pun word
is located in the second half of the sentence, and are often involved in an
`interesting' semantic, syntactic, or morphological relationship with another
word: many are antonyms, homophones, homonyms or involved in an idiom. If none
of these relationships obtain, the last word in the sentence is output.

\begin{figure}[h!]
\centering
\includegraphics[width=90mm]{decision_tree.png}
\caption{Heuristic Decision Tree Classifier}
\label{fig:method}
\end{figure}

\subsection{Bidirectional LSTM}

We implemented a Bidirection LSTM but treated the problem as a sequence labeling
task, again using 300-length glove embeddings as our word representation. We
then took the maximum prediction of that sequence and used that as the index of
the most likely word. This approach was competitive for the homographic dataset
but did not work very well for the heterographic one.

\begin{figure}[h!]
	\centering
	\includegraphics[width=110mm]{figures/lstm.png}
	\caption{Sequence-Labeling Bidirectional LSTM}
	\label{fig:LSTM}
\end{figure}

\subsection{Sliding Window based Maximum Entropy Markov Model}

We implemented a MEMM which tooks a word and a window of words preceding and
following it, as well as a number of features based on that word formed using
feature templates (see table \ref{tab:memm_features}), which output the probability
that that word was a pun or not. We run the algorithm over all words in the
sentence, again treating the problem like a sequence labeling task, and output
as the index of the word in the sentence with the highest probability of being
labeled a pun. Our intuition behind this approach was twofold: 1) it could
handle arbitrary, 2) it could handle arbitrary sentence lengths. 

\begin{table}
\caption{Sliding Window MEMM Features}\label{tab:memm_features} 
\begin{center}
\begin{tabular}{l l} 
\toprule
\textbf{Feature Name} & \textbf{Explanation} \\
\midrule
words\_remaining      & number of words after this one \\
lemma                 & the lemma, from WordNet \\
shape                 & the wordshape \\
prefix3               & first three letters \\
suffix3               & last three letters \\
word                  & the word being examined \\
pos                   & part of speech \\
stopword              & if the word is a stopword \\
en-wordlist           & if the word is in the english language \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Feature Comparision}

We iteratively ran different combinations of features using the Summit
Supercomputer [12] to identify the best set of features to use. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm]{figures/sliding_window_homographic.png}
	\caption{Sliding Window MEMM Feature Comparison (Homographic Puns, 40 iterations)}
    \label{fig:method}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm]{figures/sliding_window_heterographic.png}
	\caption{Sliding Window MEMM Feature Comparison (Heterographic Puns, 40 iterations)}
    \label{fig:method}
\end{figure}


\section{Results}
\label{results}

We used the same evaluation metrics for each of these tasks were used in
SemEval2017.

\subsection{Pun Detection}
\subsubsection{Evaluation}
This task used standard measures: accuracy, precision, recall, and F-1.
\subsubsection{Results}
here are our results. here's the baseline. here's semeval's

\subsubsection{Error Analysis}

\subsection{Pun Location}
Here's how we think we did.
\subsubsection{Evaluation}
f score etc etc
\subsubsection{Results}
here are our results. here's the baseline. here's semeval's
\subsubsection{Error Analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{conclusion}

\subsection{What Went Well}

We did particularly well at pun detection, largely leveraging the size of our
dataset over what the systems at SemEval had access to. We also were fairly
successful in trying different algorithms. 

\subsection{What We Could Have Done Better}

The best systems at SemEval applied different approaches to the problem for the
two types of data, heterographic and homographic. This makes sense, as the
characteristics of those two datasets differ significantly. Generally, our
approach would have benefitted from adding more information to our attempts at
the heterographic dataset, perhaps converting the words in a sentence into their
phonemes and identifing possible words which were close in sound.

\subsection{Future Work}

An obvious shortcoming of our system is that it will only predict \emph{one
word} as the pun word, but, and the idea that there is \emph{one word} the makes
a pun is somewhat dubious (not to mention that compound puns exist). Consider
the following pun: 
\begin{center}
	\emph{The rabbi got hit on the temple.}
\end{center}
Our system would say that the punword is ``temple'', but that is really only half
the story; if the word ``rabbi'' were replaced with, say, ``monk'', there would
be no pun. The ability to identify the words \emph{involved} in a pun would be
much more useful, and to the authors of this report, more interesting.

Of obvious interest is the ability to detect further types of puns, perhaps
using context provided by differt types of data; it would be extremely
interesting to create a system which could identify visual puns. 

It would be of interest to say \emph{what} about the words in a sentence make it
a pun. In other words, the creation of a system which could interpret puns.

A more out-there hope would be a system which could generate puns, perhaps being
given a sentence and a pair of word senses.

\subsection{Who Did What}

Project Github Repository: \url{https://github.com/hunter-/pun_classifier}

\begin{itemize}
\item Shantanu Karnwal worked on:
	\begin{itemize}
		\item decision tree classifier for pun location
		\item feature comparision for the sliding window MEMM
		\item poster layout and content
		\item report content (pun location part and images)
	\end{itemize}
\item Amir Kashipazha
	\begin{itemize}
		\item features (antonyms, idioms, sentiment)
		\item the feature comparison for the SVM used for SGD Pun Detection
	\end{itemize}
\item Brandon Boylan-Peck
	\begin{itemize}
		\item poster bar graphs
	\end{itemize}
\item Cathlyn Stone
	\begin{itemize}
		\item the interactive web app
		\item the framework for running classifiers
		\item features (lesk algorithm, POS)
		\item the SVM used in the SGD Pun Detection algorithm
		\item the sliding window MEMM used in Pun Location
		\item the ensemble algorithm
		\item algorithm cross validation
		\item the Bidirectional LSTM used for Pun Location
		\item evaluation of the Pun Detection algorithms
		\item the model caching architecture
	\end{itemize}
\item Kenneth Hunter Wapman
	\begin{itemize}
		\item data loading and manipulation
		\item the baseline algorithm for Pun Detection
		\item features (word embeddings)
		\item the framework for running classifiers
		\item the sliding window MEMM used in Pun Location
		\item the Bidirectional LSTM used for Pun Location
		\item the Bidirectional LSTM used for Pun Detection
		\item evaluation of the Pun Location algorithms
		\item poster content
		\item report content
	\end{itemize}
\end{itemize}

\subsubsection*{Acknowledgments}

We were helped immeasurably by the systems described in SemEval2017. Tyler
Scott's advice on LSTMs was also extremely helpful.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{References}

\medskip

\small

[1] Oele, D., \& Evang, K.\ (2017) BuzzSaw at SemEval-2017 Task 7:
Global vs. Local Context for Interpreting and Locating Homographic English Puns
with Sense Embeddings. Proceedings of the 11th International Workshop on
Semantic Evaluations (SemEval-2017), pages 444–448, Vancouver, Canada, August 3
- 4, 2017. 2017 © Association for Computational Linguistics.

[2] Pedersen, T.\ (2017) Duluth at SemEval-2017 Task 7: Puns Upon a Midnight
Dreary, Lexical Semantics for the Weak and Weary. Proceedings of the 11th
International Workshop on Semantic Evaluations (SemEval-2017), pages 416–420,
Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for Computational
Linguistics.

[3] Xiu, Y., Lan, M., \& Wu, Y.\ (2017). ECNU at SemEval-2017 Task 7: Using
Supervised and Unsupervised Methods to Detect and Locate English Puns.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 453–456, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[4] Hurtado, L., Segarra, E., Pla, F., Carrasco, P., \& Gonzalez, J.A..
(2017). ELiRF-UPV at SemEval-2017 Task 7: Pun Detection and Interpretation.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 440–443, Vancouver, Canada, August 3 - 4,
2017. 2017 © Association for Computational Linguistics.

[5] Indurthi, V., \&  Reddy, O.S.\ (2017). Fermi at SemEval-2017 Task
7: Detection and Interpretation of Homographic puns in English Language.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 457–460, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[6] Samuel Doogan, Aniruddha Ghosh, Hanyang Chen, and Tony Veale. 2017. Idiom
Savant at Semeval-2017 Task 7: Detection and Interpretation of English Puns.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 103–108, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[7] Das, D. \& Pramanick, A. (2017). JU\_CSE\_NLP at SemEval 2017 Task 7:
Employing Rules to Detect and Interpret English Puns. Proceedings of the 11th
International Workshop on Semantic Evaluations (SemEval-2017), pages 432–435,
Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for Computational
Linguistics.

[8] Mikhalkova, E. \& Karyakin, Y. (2017). PunFields at SemEval-2017 Task 7:
Employing Roget’s Thesaurus in Automatic Pun Recognition and Interpretation.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 426–431, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[9] Bahuleyan, H. \& Vechtomova, O. (2017). UWaterloo at SemEval-2017 Task 8:
Detecting Stance towards Rumours with Topic Independent Features. Proceedings
of the 11th International Workshop on Semantic Evaluations (SemEval-2017),
pages 461–464, Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for
Computational Linguistics.

[10] Vadehra, A. (2017). UWAV at SemEval-2017 Task 7: Automated feature-based
system for locating puns. Proceedings of the 11th International Workshop on
Semantic Evaluations (SemEval-2017), pages 449–452, Vancouver, Canada, August 3
- 4, 2017. 2017 © Association for Computational Linguistics.

[11] Sevgili, O., Ghotbi, N., \& Tekir, S. (2017). N-Hance at SemEval-2017 Task
7: A Computational Approach using Word Association for Puns. Proceedings of the
11th International Workshop on Semantic Evaluations (SemEval-2017), pages
436–439, Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for
Computational Linguistics.

[12] Jonathon Anderson, Patrick J. Burns, Daniel Milroy, Peter Ruprecht, Thomas Hauser, and Howard Jay Siegel. 2017. Deploying RMACC Summit: An HPC Resource for the Rocky Mountain Region. In Proceedings of PEARC17, New Orleans, LA, USA, July 09-13, 2017, 7 pages. DOI: 10.1145/3093338.3093379

\end{document}

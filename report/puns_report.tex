\documentclass{article}

\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{Pun Classification and Location}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	Shantanu Karnwal
	\texttt{shantanu.karnwal@colorado.edu} \\
	\And
	Amir Kashipazha
	\texttt{amirhossein.kashipazha@colorado.edu} \\
	\And
	Brandon Boylan-Peck
	\texttt{brbo9266@colorado.edu} \\
	\And
	Cathlyn Stone\\
	\texttt{cathlyn.stone@colorado.edu} \\
	\And
	Kenneth Hunter Wapman\\
	\texttt{kennethwapman@colorado.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
	Pun identification and location are challenging natural language processing 
	tasks. We implemented several algorithms for both, with results which 
	were competitive with the systems presented in the SemEval conference, where
	those tasks were originally defined.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem Description
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview}

\subsection{Pun Structure}

here we will talk about what puns are
\begin{itemize}

\item homographic
\item heterographic
\item other types

\end{itemize}

here we will talk about the problems we worked on.

\begin{center}
	homographic pun example
  % \url{https://cmt.research.microsoft.com/NIPS2017/}
\end{center}

\begin{center}
	heterographic pun example
  % \url{https://cmt.research.microsoft.com/NIPS2017/}
\end{center}

Please read carefully the instructions below and follow them
faithfully.

\subsection{Motivation}


\subsection{Tasks}

section references \ref{pun_detection}, \ref{pun_location}, and
\ref{conclusion} below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pun Detection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pun Detection}
\label{pun_detection}

here we will talk about pun detection and our algorithms.

\subsection{Baseline}

\subsection{Algorithms}

\subsection{Feature Comparison}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pun Location
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pun Location}
\label{pun_location}

\subsection{Baseline}

\subsection{Algorithms}

\subsection{Feature Comparison}
We introduced various features to improve accuracy. Table (number ?) shows the them and their indexes. These indexes are represent their related feature in feature related figures. Features were added to the code one by one to find how they improved accuracy. Accuracy change behave differently against each feature. For example, accuracy did not increased or decreased with positive and negative. In the other hand, unigram hiked up accuracy to 100 percent which means over fitting. We eliminated features which did effect accuracy from our code. Finally, we ended up using 11 features, index 0 to 10, for feature comparison.\\

In order to find what combinations of feature makes the largest accuracy, we used combination rule to find all possible combination of our 11 features. This made 2047 feature combination. We run code for each combination separately for homographic and heterographic pun, 4089 for both of them. Figures (add figure numbers here) shows the 10 largest accuracy of testing and training set for homographic and heterographic.\\



\begin{center}
\begin{tabular}{ |l|l| } 
\hline
\textbf{Index} 	&	\textbf{Feaute Name}	\\
\hline
0	&	Lesk Algorithm	\\
1	&	Pos	\\
2	&	tfidf	\\
3	&	Embeddings	\\
4	&	Unigram	\\
5	&	Number of Homophone in Pun	\\
6	&	Number of Each Homophone in Pun	\\
7	&	Homophone is in Pun or Not	\\
8	&	Idiom is in Pun or Not	\\
9	&	Antonyms is in Pun or Not	\\
10	&	Homonym is in Pun or Not	\\
11	&	Bigram	\\
12	&	Trigram	\\
13	&	Positives	\\
14	&	Negatives	\\
15	&	All or First Caps	\\
\hline
\end{tabular}
\end{center}



\begin{figure}
  \includegraphics{figures/Accuracy_on_Test_Set_for_Homographic_Pun.JPG}
  \caption{Figure NUMBER ? - Accuracy on Test Set for Homographic Pun}
  \label{fig:boat1}
\end{figure}



\section{Results}
\label{results}

\subsection{Pun Detection}
Here's how we think we did.
\subsubsection{Evaluation}
\subsubsection{Results}
here are our results. here's the baseline. here's semeval's

\subsubsection{Error Analysis}

\subsection{Pun Location}
Here's how we think we did.
\subsubsection{Evaluation}
f score etc etc
\subsubsection{Results}
here are our results. here's the baseline. here's semeval's
\subsubsection{Error Analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{conclusion}

\subsection{who did what}
\subsection{what went well}
\subsection{what we could have done better}

\subsubsection*{Acknowledgments}

here's where we acknowledge stuff

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{References}

\medskip

\small

[1] Oele, D., \& Evang, K.\ (2017) BuzzSaw at SemEval-2017 Task 7:
Global vs. Local Context for Interpreting and Locating Homographic English Puns
with Sense Embeddings. Proceedings of the 11th International Workshop on
Semantic Evaluations (SemEval-2017), pages 444–448, Vancouver, Canada, August 3
- 4, 2017. 2017 © Association for Computational Linguistics.

[2] Pedersen, T.\ (2017) Duluth at SemEval-2017 Task 7: Puns Upon a Midnight
Dreary, Lexical Semantics for the Weak and Weary. Proceedings of the 11th
International Workshop on Semantic Evaluations (SemEval-2017), pages 416–420,
Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for Computational
Linguistics.

[3] Xiu, Y., Lan, M., \& Wu, Y.\ (2017). ECNU at SemEval-2017 Task 7: Using
Supervised and Unsupervised Methods to Detect and Locate English Puns.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 453–456, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[4] Hurtado, L., Segarra, E., Pla, F., Carrasco, P., \& Gonzalez, J.A..
(2017). ELiRF-UPV at SemEval-2017 Task 7: Pun Detection and Interpretation.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 440–443, Vancouver, Canada, August 3 - 4,
2017. 2017 © Association for Computational Linguistics.

[5] Indurthi, V., \&  Reddy, O.S.\ (2017). Fermi at SemEval-2017 Task
7: Detection and Interpretation of Homographic puns in English Language.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 457–460, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[6] Samuel Doogan, Aniruddha Ghosh, Hanyang Chen, and Tony Veale. 2017. Idiom
Savant at Semeval-2017 Task 7: Detection and Interpretation of English Puns.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 103–108, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[7] Das, D. \& Pramanick, A. (2017). JU\_CSE\_NLP at SemEval 2017 Task 7:
Employing Rules to Detect and Interpret English Puns. Proceedings of the 11th
International Workshop on Semantic Evaluations (SemEval-2017), pages 432–435,
Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for Computational
Linguistics.

[8] Mikhalkova, E. \& Karyakin, Y. (2017). PunFields at SemEval-2017 Task 7:
Employing Roget’s Thesaurus in Automatic Pun Recognition and Interpretation.
Proceedings of the 11th International Workshop on Semantic Evaluations
(SemEval-2017), pages 426–431, Vancouver, Canada, August 3 - 4, 2017. 2017 ©
Association for Computational Linguistics.

[9] Bahuleyan, H. \& Vechtomova, O. (2017). UWaterloo at SemEval-2017 Task 8:
Detecting Stance towards Rumours with Topic Independent Features. Proceedings
of the 11th International Workshop on Semantic Evaluations (SemEval-2017),
pages 461–464, Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for
Computational Linguistics.

[10] Vadehra, A. (2017). UWAV at SemEval-2017 Task 7: Automated feature-based
system for locating puns. Proceedings of the 11th International Workshop on
Semantic Evaluations (SemEval-2017), pages 449–452, Vancouver, Canada, August 3
- 4, 2017. 2017 © Association for Computational Linguistics.

[11] Sevgili, O., Ghotbi, N., \& Tekir, S. (2017). N-Hance at SemEval-2017 Task
7: A Computational Approach using Word Association for Puns. Proceedings of the
11th International Workshop on Semantic Evaluations (SemEval-2017), pages
436–439, Vancouver, Canada, August 3 - 4, 2017. 2017 © Association for
Computational Linguistics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Misc Style examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Style Stuff}
\verb+this is code-ish looking+ 

here's a percent: $\sim$$15\%$
\textbf{boldboldbold}
\emph{italics}

here's a fraction: \nicefrac{1}{4}

\paragraph{}This is paragraphed
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

ref number:\ [4]

footnotes.\footnote{they go after the period}

\begin{figure}[h]
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}

Table~\ref{sample-table}.

use \verb+booktabs+ package for tables
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}

\begin{table}[t]
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule{1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\end{document}
